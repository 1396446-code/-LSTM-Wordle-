import pandas as pd
import numpy as np
import gensim.downloader as api
import os
import sys

# ==========================================
# 1. è‡ªåŠ¨è·¯å¾„å®šä½å·¥å…· (é˜²æ­¢æ‰¾ä¸åˆ°æ–‡ä»¶)
# ==========================================
def get_file_path(filename):
    """
    æ™ºèƒ½å¯»æ‰¾æ–‡ä»¶ï¼šä¼˜å…ˆåœ¨è„šæœ¬æ‰€åœ¨ç›®å½•æ‰¾ï¼Œå…¶æ¬¡åœ¨å½“å‰å·¥ä½œç›®å½•æ‰¾ã€‚
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # è·¯å¾„ 1: è„šæœ¬åŒçº§ç›®å½•
    path_in_script_dir = os.path.join(script_dir, filename)
    if os.path.exists(path_in_script_dir):
        return path_in_script_dir
    
    # è·¯å¾„ 2: è¿è¡Œæ—¶çš„å½“å‰ç›®å½•
    if os.path.exists(filename):
        return os.path.abspath(filename)
        
    return None

# ==========================================
# 2. æ ¸å¿ƒç±»: è¯å‘é‡ç”Ÿæˆå™¨
# ==========================================
class EmbeddingGenerator:
    def __init__(self, csv_path):
        print(f"-->æ­£åœ¨è¯»å–é¢„å¤„ç†æ•°æ®: {csv_path}")
        self.df = pd.read_csv(csv_path)
        
        # æå–ç›®æ ‡å•è¯åˆ— (Target_Word)ï¼Œè½¬å°å†™å¹¶å»ç©ºæ ¼
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªä¸º Target Word ç”Ÿæˆå‘é‡ï¼Œå› ä¸ºè¿™æ˜¯ LSTM çš„æ ¸å¿ƒè¾“å…¥
        self.words = self.df['Target_Word'].astype(str).str.strip().str.lower().values
        self.vectors = []
        
        # åµŒå…¥ç»´åº¦ (GloVe 100ç»´)
        self.embedding_dim = 100 
        
    def load_pretrained_model(self, model_name="glove-wiki-gigaword-100"):
        """
        åŠ è½½ Gensim çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
        å‚æ•° model_name: 
          - 'glove-wiki-gigaword-100' (æ¨è: 100ç»´, çº¦128MB, é€Ÿåº¦å¿«æ•ˆæœå¥½)
          - 'word2vec-google-news-300' (æœ€å¼º: 300ç»´, çº¦1.6GB, ä¸‹è½½æ…¢)
        """
        print(f"\n[1/3] æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ '{model_name}'...")
        print("      (é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...)")
        
        try:
            self.model = api.load(model_name)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! (è¯è¡¨å¤§å°: {len(self.model.index_to_key)})")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚å¦‚æœä¸‹è½½å¤ªæ…¢ï¼Œå¯ä»¥å°è¯•æ›´æ¢ç½‘ç»œæˆ–ä½¿ç”¨ VPNã€‚")
            sys.exit(1)

    def generate_embedding_matrix(self):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šéå†æˆ‘ä»¬çš„å•è¯è¡¨ï¼Œå»é¢„è®­ç»ƒæ¨¡å‹é‡ŒæŸ¥è¡¨
        """
        print(f"\n[2/3] æ­£åœ¨ä¸º {len(self.words)} ä¸ªå•è¯ç”Ÿæˆå‘é‡...")
        
        matrix_list = []
        found_count = 0
        oov_count = 0 # Out-Of-Vocabulary (æœªç™»å½•è¯)
        
        for word in self.words:
            if word in self.model:
                # æƒ…å†µ A: å•è¯åœ¨å­—å…¸é‡Œ -> ç›´æ¥è·å–å‘é‡
                vec = self.model[word]
                found_count += 1
            else:
                # æƒ…å†µ B: å•è¯ä¸åœ¨å­—å…¸é‡Œ (ç”Ÿåƒ»è¯) -> ä½¿ç”¨éšæœºå‘é‡åˆå§‹åŒ–
                # ä¿æŒä¸é¢„è®­ç»ƒå‘é‡ç›¸ä¼¼çš„åˆ†å¸ƒ (å‡å€¼0, æ–¹å·®0.6)
                vec = np.random.normal(scale=0.6, size=(self.embedding_dim,))
                oov_count += 1
                print(f"   âš ï¸ [ç”Ÿåƒ»è¯å‘ç°] '{word}' æœªåœ¨æ¨¡å‹ä¸­æ‰¾åˆ°ï¼Œå·²ä½¿ç”¨éšæœºå‘é‡ä»£æ›¿ã€‚")
            
            matrix_list.append(vec)
            
        # è½¬æ¢ä¸º NumPy çŸ©é˜µ (Shape: N_samples x Embedding_dim)
        self.vectors = np.array(matrix_list)
        
        print(f"\n--> ç»Ÿè®¡ç»“æœ:")
        print(f"    âœ… å®Œç¾åŒ¹é…: {found_count} ä¸ª")
        print(f"    âš ï¸ æœªç™»å½•è¯: {oov_count} ä¸ª")
        print(f"    ç”Ÿæˆçš„çŸ©é˜µå½¢çŠ¶: {self.vectors.shape} (è¡Œæ•°åº”ç­‰äº {len(self.words)})")

    def save_results(self, output_filename='word_embeddings.npy'):
        """
        ä¿å­˜ç»“æœä¸º .npy æ–‡ä»¶
        """
        print(f"\n[3/3] ä¿å­˜ç»“æœ...")
        script_dir = os.path.dirname(os.path.abspath(__file__))
        save_path = os.path.join(script_dir, output_filename)
        
        np.save(save_path, self.vectors)
        print(f"ğŸ‰ æˆåŠŸ! è¯å‘é‡çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")
        print("------------------------------------------------")
        print("ã€äº¤ä»˜æŒ‡å—ã€‘")
        print("è¯·å°† 'word_embeddings.npy' å‘ç»™ Member Aã€‚")
        print("å¹¶åœ¨æŠ¥å‘Šä¸­è¯´æ˜ï¼š'ä½¿ç”¨äº†åŸºäº Wikipedia è¯­æ–™åº“è®­ç»ƒçš„ GloVe-100d æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œä»¥ç¡®ä¿è¾“å…¥å±‚çš„è¯­ä¹‰è¡¨è¾¾è´¨é‡ã€‚'")

# ==========================================
# 3. ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # ä½ çš„é¢„å¤„ç†æ–‡ä»¶
    input_csv = 'wordle_preprocessed_final.csv'
    
    # è·å–æ–‡ä»¶è·¯å¾„
    file_path = get_file_path(input_csv)
    
    if file_path:
        # å®ä¾‹åŒ–å¹¶è¿è¡Œ
        generator = EmbeddingGenerator(file_path)
        
        # æ­¥éª¤ 1: åŠ è½½ GloVe æ¨¡å‹
        generator.load_pretrained_model("glove-wiki-gigaword-100")
        
        # æ­¥éª¤ 2: ç”Ÿæˆå‘é‡
        generator.generate_embedding_matrix()
        
        # æ­¥éª¤ 3: ä¿å­˜
        generator.save_results()
        
    else:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{input_csv}'ã€‚")
        print("è¯·ç¡®ä¿ä½ å·²ç»å®Œæˆäº†ä¸Šä¸€æ­¥çš„æ–‡æœ¬é¢„å¤„ç†ï¼Œå¹¶ä¸”æ–‡ä»¶å°±åœ¨å½“å‰ç›®å½•ä¸‹ã€‚")